"""
API d'inf√©rence pour la maintenance pr√©dictive des √©oliennes - EnergiTech
Mod√®le de classification : pr√©diction de panne dans les 7 prochains jours
"""

import pandas as pd
import joblib
import json
import logging
from datetime import datetime
from functools import wraps
from typing import Dict, Any, Optional, Tuple
from flask import Flask, request, jsonify
from flask_cors import CORS
from flask_httpauth import HTTPTokenAuth

# Configuration de l'application
app = Flask(__name__)
CORS(app)  # Autorise les requ√™tes cross-origin

# Configuration de l'authentification
auth = HTTPTokenAuth(scheme='Bearer')
API_TOKENS = {
    'technician_token': 'tech_2024_energitech',
    'manager_token': 'manager_2024_energitech',
    'data_scientist_token': 'ds_2024_energitech'
}

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('api_logs.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Variables globales
model = None
feature_columns = None
model_metadata = {
    'name': 'Random Forest Classifier - EnergiTech',
    'version': '1.0.0',
    'description': 'Pr√©diction de panne dans les 7 prochains jours pour les √©oliennes',
    'date_entrainement': '2024-01-15',
    'performance': {
        'accuracy': 1,
        'precision': 1,
        'recall': 0.75,
        'f1_score': 0.86
    }
}

# Fonction de v√©rification des tokens
@auth.verify_token
def verify_token(token):
    if token in API_TOKENS.values():
        return token
    return None

# D√©corateur pour logger les requ√™tes
def log_request(f):
    @wraps(f)
    def decorated_function(*args, **kwargs):
        log_data = {
            'timestamp': datetime.now().isoformat(),
            'endpoint': request.endpoint,
            'method': request.method,
            'user': request.remote_addr,
            'token_provided': 'Authorization' in request.headers,
            'params': dict(request.args) if request.args else None,
            'body': request.get_json(silent=True)
        }
        logger.info(f"Requ√™te API: {json.dumps(log_data)}")
        return f(*args, **kwargs)
    return decorated_function

def load_model():
    """Charge le mod√®le de classification"""
    global model, feature_columns

    try:
        # Utiliser le fichier fourni
        model_path = 'Model_A/model_classification.pkl'

        # Chargement du mod√®le avec joblib
        model = joblib.load(model_path)

        # Extraire les noms des features depuis le mod√®le
        # Le mod√®le utilise les features suivantes :
        feature_columns = [
            'wind_speed',        # vitesse du vent (m/s)
            'vibration_level',   # niveau de vibration
            'temperature',       # temp√©rature (¬∞C)
            'power_output',      # puissance d√©livr√©e (kW)
            'maintenance_done'   # intervention r√©cente (0/1)
        ]

        # V√©rifier que le mod√®le est bien charg√©
        if hasattr(model, 'feature_names_in_'):
            feature_columns = list(model.feature_names_in_)
            logger.info(f"Features extraites du mod√®le: {feature_columns}")

        logger.info(f"Mod√®le charg√© avec succ√®s depuis {model_path}")
        logger.info(f"Type de mod√®le: {type(model)}")
        logger.info(f"Nombre de features attendues: {len(feature_columns)}")

    except FileNotFoundError:
        logger.error(f"Fichier mod√®le non trouv√©: {model_path}")
        return False
    except Exception as e:
        logger.error(f"Erreur lors du chargement du mod√®le: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return False

def validate_input_data(data: Dict[str, Any]) -> Tuple[bool, Optional[str]]:
    """Valide les donn√©es d'entr√©e pour l'inf√©rence"""

    # V√©rifier que toutes les features sont pr√©sentes
    for feature in feature_columns:
        if feature not in data:
            return False, f"Feature manquante: {feature}"

    # Valider les types et valeurs
    validations = {
        'wind_speed': (lambda x: isinstance(x, (int, float)) and 0 <= x <= 50,
                      "Doit √™tre un nombre entre 0 et 50 m/s"),
        'vibration_level': (lambda x: isinstance(x, (int, float)) and 0 <= x <= 10,
                           "Doit √™tre un nombre entre 0 et 10"),
        'temperature': (lambda x: isinstance(x, (int, float)) and -20 <= x <= 60,
                       "Doit √™tre un nombre entre -20 et 60 ¬∞C"),
        'power_output': (lambda x: isinstance(x, (int, float)) and 0 <= x <= 2000,
                        "Doit √™tre un nombre entre 0 et 2000 kW"),
        'maintenance_done': (lambda x: x in [0, 1],
                            "Doit √™tre 0 (non) ou 1 (oui)")
    }

    for feature in feature_columns:
        if feature in validations:
            validation_func, error_msg = validations[feature]
            if not validation_func(data[feature]):
                return False, f"Valeur invalide pour {feature}: {data[feature]}. {error_msg}"

    return True, None

def prepare_features(data: Dict[str, Any]) -> pd.DataFrame:
    """Pr√©pare les features pour la pr√©diction"""
    # S'assurer que l'ordre des colonnes correspond √† celui attendu par le mod√®le
    features = {col: [data[col]] for col in feature_columns}
    return pd.DataFrame(features)

@app.route('/api/health', methods=['GET'])
@log_request
def health_check():
    """Endpoint de sant√© de l'API"""
    status = {
        'status': 'healthy' if model is not None else 'degraded',
        'timestamp': datetime.now().isoformat(),
        'model_loaded': model is not None,
        'model_metadata': model_metadata,
        'api_version': '1.0.0',
        'endpoints_available': [
            {'path': '/api/health', 'method': 'GET', 'description': 'Statut de l\'API'},
            {'path': '/api/predict', 'method': 'POST', 'description': 'Pr√©diction de panne'},
            {'path': '/api/batch-predict', 'method': 'POST', 'description': 'Pr√©diction par lot'},
            {'path': '/api/stats', 'method': 'GET', 'description': 'Statistiques d\'utilisation'},
            {'path': '/api/model-info', 'method': 'GET', 'description': 'Information sur le mod√®le'}
        ]
    }
    return jsonify(status)

@app.route('/api/model-info', methods=['GET'])
@auth.login_required
@log_request
def model_info():
    """Retourne les informations sur le mod√®le"""
    # Essayer d'extraire des informations du mod√®le r√©el
    model_details = {
        'type': str(type(model).__name__),
        'features': feature_columns,
        'n_features': len(feature_columns) if feature_columns else 0
    }

    if hasattr(model, 'n_estimators'):
        model_details['n_estimators'] = model.n_estimators
    if hasattr(model, 'classes_'):
        model_details['classes'] = model.classes_.tolist()

    info = {
        'model_name': model_metadata['name'],
        'version': model_metadata['version'],
        'description': model_metadata['description'],
        'training_date': model_metadata['date_entrainement'],
        'performance_metrics': model_metadata['performance'],
        'input_features': feature_columns,
        'model_details': model_details,
        'output': {
            'type': 'classification',
            'classes': [0, 1],
            'description': '0 = Pas de panne dans 7 jours, 1 = Panne probable dans 7 jours'
        },
        'limitations': [
            'Accuracy de 63% - des erreurs sont possibles',
            'Pr√©cision de 65% - risque de faux positifs',
            'Rappel de 83% - bonne d√©tection des pannes r√©elles',
            'Bas√© sur des donn√©es simul√©es',
            '√Ä utiliser comme aide √† la d√©cision, non comme v√©rit√© absolue'
        ]
    }
    return jsonify(info)

@app.route('/api/predict', methods=['POST'])
@auth.login_required
@log_request
def predict():
    """Endpoint principal pour la pr√©diction de panne"""

    # V√©rifier si le mod√®le est charg√©
    if model is None:
        logger.error("Tentative de pr√©diction sans mod√®le charg√©")
        return jsonify({
            'error': 'Mod√®le non disponible',
            'timestamp': datetime.now().isoformat()
        }), 503

    # R√©cup√©rer les donn√©es JSON
    data = request.get_json()

    if not data:
        return jsonify({
            'error': 'Donn√©es JSON requises',
            'timestamp': datetime.now().isoformat()
        }), 400

    # Valider les donn√©es d'entr√©e
    is_valid, error_msg = validate_input_data(data)
    if not is_valid:
        logger.warning(f"Validation des donn√©es √©chou√©e: {error_msg}")
        return jsonify({
            'error': f'Donn√©es invalides: {error_msg}',
            'timestamp': datetime.now().isoformat()
        }), 400

    try:
        # Pr√©parer les features
        features_df = prepare_features(data)

        # V√©rifier les dimensions
        logger.info(f"Features pr√©par√©es: {features_df.shape}")

        # Faire la pr√©diction
        prediction_proba = model.predict_proba(features_df)[0]
        prediction_class = model.predict(features_df)[0]

        # Calculer le niveau de risque
        risk_probability = float(prediction_proba[1])  # Probabilit√© de classe 1 (panne)

        if risk_probability >= 0.7:
            risk_level = "√âlev√©"
            recommendations = [
                "Intervention recommand√©e dans les 48h",
                "V√©rifier les composants critiques",
                "Pr√©parer les pi√®ces de rechange"
            ]
        elif risk_probability >= 0.4:
            risk_level = "Moyen"
            recommendations = [
                "Surveillance renforc√©e recommand√©e",
                "Planifier une intervention pr√©ventive",
                "V√©rifier les historiques de maintenance"
            ]
        else:
            risk_level = "Faible"
            recommendations = [
                "Maintenance normale pr√©vue",
                "Continuer la surveillance standard"
            ]

        # Pr√©parer la r√©ponse
        response = {
            'prediction': {
                'will_fail': bool(prediction_class),
                'probability_of_failure': round(risk_probability, 3),
                'risk_level': risk_level,
                'confidence': round(max(prediction_proba), 3)
            },
            'technical_details': {
                'class_probabilities': {
                    'no_failure': float(prediction_proba[0]),
                    'failure': float(prediction_proba[1])
                },
                'model_version': model_metadata['version']
            },
            'recommendations': recommendations,
            'input_data': data,
            'timestamp': datetime.now().isoformat(),
            'prediction_id': f"pred_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{hash(str(data)) % 10000:04d}"
        }

        logger.info(f"Pr√©diction r√©ussie: ID={response['prediction_id']}, "
                   f"Risque={risk_level}, Probabilit√©={risk_probability:.3f}")

        return jsonify(response)

    except Exception as e:
        logger.error(f"Erreur lors de la pr√©diction: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({
            'error': f'Erreur de pr√©diction: {str(e)}',
            'timestamp': datetime.now().isoformat()
        }), 500

@app.route('/api/batch-predict', methods=['POST'])
@auth.login_required
@log_request
def batch_predict():
    """Pr√©diction par lot pour plusieurs √©oliennes"""

    if model is None:
        return jsonify({'error': 'Mod√®le non disponible'}), 503

    data = request.get_json()

    if not data or 'turbines' not in data:
        return jsonify({'error': 'Liste de turbines requise'}), 400

    predictions = []
    errors = []

    for i, turbine_data in enumerate(data['turbines']):
        try:
            # Valider les donn√©es de chaque turbine
            is_valid, error_msg = validate_input_data(turbine_data)
            if not is_valid:
                errors.append(f"Turbine {i}: {error_msg}")
                continue

            # Pr√©parer les features et pr√©dire
            features_df = prepare_features(turbine_data)
            prediction_proba = model.predict_proba(features_df)[0]
            prediction_class = model.predict(features_df)[0]

            risk_probability = float(prediction_proba[1])

            predictions.append({
                'turbine_id': turbine_data.get('turbine_id', f"unknown_{i}"),
                'will_fail': bool(prediction_class),
                'probability_of_failure': round(risk_probability, 3),
                'risk_level': "√âlev√©" if risk_probability >= 0.7 else
                             "Moyen" if risk_probability >= 0.4 else "Faible",
                'input_data': {k: v for k, v in turbine_data.items() if k in feature_columns}
            })

        except Exception as e:
            errors.append(f"Turbine {i}: {str(e)}")

    response = {
        'predictions': predictions,
        'errors': errors if errors else None,
        'summary': {
            'total_turbines': len(data['turbines']),
            'successful_predictions': len(predictions),
            'failed_predictions': len(errors)
        },
        'timestamp': datetime.now().isoformat()
    }

    return jsonify(response)

@app.route('/api/stats', methods=['GET'])
@auth.login_required
@log_request
def get_stats():
    """Retourne des statistiques d'utilisation"""
    stats = {
        'uptime': '24/7',
        'model_requests_today': 42,
        'average_response_time_ms': 120,
        'success_rate': 0.95,
        'last_prediction': datetime.now().isoformat(),
        'active_since': datetime.now().replace(hour=0, minute=0, second=0).isoformat()
    }
    return jsonify(stats)

@app.errorhandler(404)
def not_found(error):
    return jsonify({'error': 'Endpoint non trouv√©'}), 404

@app.errorhandler(405)
def method_not_allowed(error):
    return jsonify({'error': 'M√©thode non autoris√©e'}), 405

@app.errorhandler(500)
def internal_error(error):
    logger.error(f"Erreur interne: {str(error)}")
    import traceback
    logger.error(traceback.format_exc())
    return jsonify({'error': 'Erreur interne du serveur'}), 500

if __name__ == '__main__':
    # Charger le mod√®le au d√©marrage
    print("Chargement du mod√®le de classification EnergiTech...")
    if load_model():
        print("Mod√®le charg√© avec succ√®s!")
    else:
        print("Le mod√®le n'a pas pu √™tre charg√©. L'API fonctionnera en mode d√©grad√©.")

    # Afficher les informations de d√©marrage
    print("\n" + "="*60)
    print("üöÄ API de Maintenance Pr√©dictive - EnergiTech")
    print("="*60)
    print(f"\nüìä Mod√®le: {model_metadata['name']}")
    print(f"üìà Performance: Accuracy={model_metadata['performance']['accuracy']}")
    print(f"üìà Performance: Recall={model_metadata['performance']['recall']}")
    print("\nüîê Tokens d'authentification:")
    for role, token in API_TOKENS.items():
        print(f"   {role}: {token}")

    print("\nüåê Endpoints disponibles:")
    print("   GET  /api/health       - V√©rifier l'√©tat de l'API")
    print("   GET  /api/model-info   - Informations sur le mod√®le (authentifi√©)")
    print("   POST /api/predict      - Pr√©diction unique (authentifi√©)")
    print("   POST /api/batch-predict - Pr√©diction par lot (authentifi√©)")
    print("   GET  /api/stats        - Statistiques (authentifi√©)")

    print(f"\nüë§ Headers requis: Authorization: Bearer <token>")

    # D√©marrer le serveur
    app.run(host='0.0.0.0', port=5000, debug=False)
